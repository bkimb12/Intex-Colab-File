{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Finished Classification Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# make sure to install these packages before running:\n",
        "!pip install sodapy\n",
        "\n",
        "import pandas as pd\n",
        "from sodapy import Socrata\n",
        "import requests\n",
        "import json"
      ],
      "metadata": {
        "id": "oRvgvRLpUI8Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a2f9591-9a47-4711-b13c-1be3ec29957a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sodapy\n",
            "  Downloading sodapy-2.1.0-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from sodapy) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->sodapy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->sodapy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->sodapy) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->sodapy) (1.24.3)\n",
            "Installing collected packages: sodapy\n",
            "Successfully installed sodapy-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#In this cell, we include the functions that we might need to use as we try to run MLR.\n",
        "\n",
        "#Read Data\n",
        "def get_data(records = 2000):\n",
        "  import pandas as pd\n",
        "  from sodapy import Socrata\n",
        "  import requests\n",
        "  import json\n",
        "\n",
        "  # Unauthenticated client only works with public data sets. Note 'None'\n",
        "  # in place of application token, and no username or password:\n",
        "  client = Socrata(\"opendata.utah.gov\", 'GkoHUxXZ1JrjaFB0FlLHpniwf')\n",
        "\n",
        "  # First 2000 results, returned as JSON from API / converted to Python list of\n",
        "  # dictionaries by sodapy.\n",
        "  results = client.get(\"herb-zqda\", limit = records)\n",
        "\n",
        "  # Convert to pandas DataFrame\n",
        "  df = pd.DataFrame.from_records(results)\n",
        "\n",
        "  return df\n",
        "\n",
        "# Handle missing data\n",
        "\n",
        "def drop_columns_missing_data(df, percent = 0.5):\n",
        "  for col in df:\n",
        "    if (df[col].isna().sum() / len(df)) > percent:\n",
        "      df.drop(columns = [col], inplace = True)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "Fbd3kurXr0I1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This will collect all of the data into a dataframe that we will alter below\n",
        "df_all = get_data(252500)"
      ],
      "metadata": {
        "id": "7B7PbMuotDbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#First we set df = df_all. This helps us if we don't want to get the data from the API everytime, allowing for faster subsequent queries.\n",
        "df = df_all"
      ],
      "metadata": {
        "id": "xNhyGY0uwF5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "#This creates a new column, generated from crash_datetime that shows the time of day that the crash occurred\n",
        "df['crash_time'] = pd.to_datetime(df['crash_datetime'], format='%Y-%m-%d %H:%M:%S.%f').dt.hour\n",
        "\n",
        "#This creates a new column, generated from crash_datetime that shows the day of the month that the crash occurred\n",
        "df['crash_day'] = pd.to_datetime(df['crash_datetime'], format='%Y-%m-%d %H:%M:%S.%f').dt.day\n",
        "\n",
        "#This creates a new column, generated from crash_datetime that shows the day of the week that the crash occurred\n",
        "df['crash_dayofweek'] = pd.to_datetime(df['crash_datetime'], format='%Y-%m-%d %H:%M:%S.%f').dt.dayofweek\n",
        "\n",
        "#This creates a new column, generated from crash_datetime that shows the year that the crash occurred\n",
        "df['crash_year'] = pd.to_datetime(df['crash_datetime'], format='%Y-%m-%d %H:%M:%S.%f').dt.year\n",
        "\n",
        "#This creates a new column, generated from crash_datetime that shows the month that the crash occurred\n",
        "df['crash_month'] = pd.to_datetime(df['crash_datetime'], format='%Y-%m-%d %H:%M:%S.%f').dt.month"
      ],
      "metadata": {
        "id": "Yg_pLwEZIN2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sqlalchemy import create_engine\n",
        "import urllib.parse\n",
        "urllib.parse.quote_plus(\"http://aa1zdq2gijc4ui.c1dtnhbcknoc.us-east-1.rds.amazonaws.com/\")\n",
        "!pip install mysql-connector-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwXGr8V1ONnH",
        "outputId": "31d10d23-7277-450e-a151-080d03a1f236"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mysql-connector-python\n",
            "  Downloading mysql_connector_python-8.0.28-cp37-cp37m-manylinux1_x86_64.whl (37.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 37.6 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from mysql-connector-python) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.0.0->mysql-connector-python) (1.15.0)\n",
            "Installing collected packages: mysql-connector-python\n",
            "Successfully installed mysql-connector-python-8.0.28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Here we drop any columns that we won't be using as part of our regression (mainly ones that will not work as dummy codes by nature)\n",
        "df = df.drop(columns=['main_road_name','long_utm_x','lat_utm_y','milepoint','route', 'crash_id', 'crash_day', 'crash_datetime', 'city', 'county_name'])"
      ],
      "metadata": {
        "id": "ZWrS2ryyaqO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The first function called drops any columns with more than 50% of it's data missing (this is not important needed here, since no columns are missing \n",
        "#that much data, but it could be useful with future datasets). The second function drops any rows in which the crash_severity_id (which will be the \n",
        "#label) is missing.\n",
        "\n",
        "df = drop_columns_missing_data(df)\n",
        "df = df.dropna(subset=['crash_severity_id'])\n",
        "df = df.dropna(subset=['work_zone_related'])"
      ],
      "metadata": {
        "id": "jl4xttloxMur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This is helpful to show the distrubution of crashes in the dataset by varying levels of severity\n",
        "\n",
        "df['crash_severity_id'].value_counts()"
      ],
      "metadata": {
        "id": "KFIqJMV7xkXB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4369455-f79a-41ab-bf97-5e6a6eab3c27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    177189\n",
              "2     43871\n",
              "3     23889\n",
              "4      4652\n",
              "5       960\n",
              "Name: crash_severity_id, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 266
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For any other variable that has missing values (in this case it is only the work_zone_related field), this guesses the variable for that column. It\n",
        "#also creates dummy codes for the other non-numeric variables. This also changes crash severity id to a float, so we need to make sure that is changed back.\n",
        "#This is done in the next cell down\n",
        "#df = impute_mean(df, 'crash_severity_id')\n",
        "\n",
        "for col in df:\n",
        "  if not pd.api.types.is_numeric_dtype(df[col]) and col != 'crash_severity_id':\n",
        "    df = pd.get_dummies(df, columns=[col], drop_first=True)"
      ],
      "metadata": {
        "id": "T6m6qut0xOnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Imports decision tree tools from sklearn\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split dataset in features and target variable\n",
        "\n",
        "y = df['crash_severity_id'] # Label\n",
        "X = df.drop(columns=['crash_severity_id']) # Features\n",
        "\n",
        "# Split dataset into training set and test set\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test\n",
        "\n",
        "# Create Decision Tree classifer object\n",
        "clf = DecisionTreeClassifier()\n",
        "\n",
        "# Train Decision Tree Classifer\n",
        "clf = clf.fit(X_train,y_train)\n",
        "\n",
        "# Predict the labels for test dataset\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# View the predicted versus actual in a DataFrame\n",
        "output_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred,})\n",
        "print(output_df.head(10))\n",
        "\n",
        "print(output_df['Predicted'].value_counts())\n",
        "print(output_df['Actual'].value_counts())"
      ],
      "metadata": {
        "id": "NhE0iJHy4en8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fe05abd-ba69-4fa9-d3f3-ea23de21f0d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Actual Predicted\n",
            "252458      4         2\n",
            "162436      1         2\n",
            "107983      1         1\n",
            "119121      1         1\n",
            "136778      1         1\n",
            "221811      3         1\n",
            "105312      3         1\n",
            "130930      1         1\n",
            "195936      1         3\n",
            "43687       4         2\n",
            "1    59779\n",
            "2     8829\n",
            "3     5110\n",
            "4     1154\n",
            "5      297\n",
            "Name: Predicted, dtype: int64\n",
            "1    53296\n",
            "2    13138\n",
            "3     7042\n",
            "4     1398\n",
            "5      295\n",
            "Name: Actual, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 12345)\n",
        "\n",
        "clf = DecisionTreeClassifier()\n",
        "clf = clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(f\"Accuracy:\\t{metrics.accuracy_score(y_test, y_pred)}\")"
      ],
      "metadata": {
        "id": "EMiWpVB2OukV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ee3fb42-6225-4c53-b3e3-520ab0cbb7a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:\t0.6341177879178916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHcs7AnP8hug",
        "outputId": "0ff4d862-d806-438c-89f8-48f0b22f43d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.75      0.84      0.79     53204\n",
            "           2       0.20      0.14      0.16     13010\n",
            "           3       0.22      0.15      0.18      7253\n",
            "           4       0.12      0.11      0.11      1373\n",
            "           5       0.11      0.09      0.10       329\n",
            "\n",
            "    accuracy                           0.63     75169\n",
            "   macro avg       0.28      0.26      0.27     75169\n",
            "weighted avg       0.59      0.63      0.61     75169\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Generate class membership probabilities\n",
        "y_preb_probs = clf.predict_proba(X_test)\n",
        "\n",
        "roc_auc_score(\n",
        "    y_test, y_preb_probs, average=\"weighted\", multi_class=\"ovr\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TEd_rFk_Tjy",
        "outputId": "2062f39b-54c1-4f53-9398-4c5275bdcd38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5658563016574645"
            ]
          },
          "metadata": {},
          "execution_count": 271
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "cohen_kappa_score(y_test, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOu1mxiH_nbw",
        "outputId": "6b6a015b-1ff4-4ddf-fa34-14b7f8f1f4fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1083563768505148"
            ]
          },
          "metadata": {},
          "execution_count": 272
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_corrcoef(y_test, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_j2_rnMi_tqE",
        "outputId": "25cf93e9-82b5-4516-9489-385592f83bfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.11090578850799501"
            ]
          },
          "metadata": {},
          "execution_count": 273
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import log_loss\n",
        "\n",
        "log_loss(y_test, y_preb_probs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgmCEkkp_09Y",
        "outputId": "0897f99b-9e60-47ea-c7d3-45ba9060784c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.339658569318308"
            ]
          },
          "metadata": {},
          "execution_count": 274
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_test = [\n",
        "            [10.0, 4.0, 2019.0, 2.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n",
        "            [22.0, 0.0, 2016.0, 12.0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1],\n",
        "            [12.0, 3.0, 2019.0, 2.0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
        "]\n",
        "\n",
        "new_prediction = clf.predict(new_test)"
      ],
      "metadata": {
        "id": "nztZdEqrB_eS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "097d42ce-6c75-4540-847f-14af3e8eb854"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:451: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
            "  \"X does not have valid feature names, but\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(new_prediction)"
      ],
      "metadata": {
        "id": "3iifcdPsCVIX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0f04152-f913-43df-a892-a526cd75a202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1' '4' '5']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install skl2onnx\n",
        "!pip install onnxruntime"
      ],
      "metadata": {
        "id": "x28jDYtUPJuu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97dc46c8-e4ba-44ec-e4ef-2a449bdd8151"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: skl2onnx in /usr/local/lib/python3.7/dist-packages (1.11.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19 in /usr/local/lib/python3.7/dist-packages (from skl2onnx) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from skl2onnx) (1.21.5)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from skl2onnx) (3.17.3)\n",
            "Requirement already satisfied: onnxconverter-common>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from skl2onnx) (1.9.0)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from skl2onnx) (1.4.1)\n",
            "Requirement already satisfied: onnx>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from skl2onnx) (1.11.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx>=1.2.1->skl2onnx) (3.10.0.2)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->skl2onnx) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19->skl2onnx) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19->skl2onnx) (3.1.0)\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.7/dist-packages (1.11.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from onnxruntime) (1.21.5)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from onnxruntime) (2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from onnxruntime) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->onnxruntime) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from skl2onnx import convert_sklearn\n",
        "from skl2onnx.common.data_types import FloatTensorType\n",
        "import onnxruntime as rt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "SNoKLdV_TDdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converts all dtypes to type float\n",
        "for col in df:\n",
        "  if col != 'crash_severity_id':\n",
        "    df[col] = df[col].astype(float)"
      ],
      "metadata": {
        "id": "we7raoTSWhid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ONNXModelPath = \"../crash_model.onnx\"\n",
        "\n",
        "num_features = 23\n",
        "\n",
        "initial_type = [('float_input', FloatTensorType([None, num_features]))]\n",
        "onnx = convert_sklearn(clf, initial_types=initial_type)\n",
        "with open(ONNXModelPath, \"wb\") as f:\n",
        "    f.write(onnx.SerializeToString())"
      ],
      "metadata": {
        "id": "GjqaFOYuV0vR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sess = rt.InferenceSession(ONNXModelPath)\n",
        "input_name = sess.get_inputs()[0].name\n",
        "label_name = sess.get_outputs()[0].name\n",
        "pred_onx = sess.run(None, {input_name: X_train.values.astype(np.float32)})[0]\n",
        "print(pred_onx)"
      ],
      "metadata": {
        "id": "-41mvQDJY-Qj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98163e6a-a51a-4957-877d-9d1554c4a0ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1' '1' '1' ... '3' '2' '1']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NewValue = np.array([[12.0, 3.0, 2019.0, 2.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
        "sess.run(None, {input_name: NewValue.astype(np.float32)})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tkt9ebgkgCoo",
        "outputId": "5d6a85da-6292-4b84-c3e6-b214907c4033"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array(['1'], dtype=object),\n",
              " [{'1': 0.6000000238418579,\n",
              "   '2': 0.20000000298023224,\n",
              "   '3': 0.20000000298023224,\n",
              "   '4': 0.0,\n",
              "   '5': 0.0}]]"
            ]
          },
          "metadata": {},
          "execution_count": 290
        }
      ]
    }
  ]
}